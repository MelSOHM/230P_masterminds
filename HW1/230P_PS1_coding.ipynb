{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qY3-0HAWG71"
      },
      "source": [
        "# Financial Time Series Dataset\n",
        "\n",
        "## Overview\n",
        "This dataset contains **raw** daily financial and macroeconomic data for 100 firms over 121 trading days. The dataset provides fundamental features that serve as building blocks for feature engineering and machine learning applications in financial markets. Researchers are expected to construct higher-level features (interactions, lags, transformations) from these raw inputs.\n",
        "\n",
        "## Dataset Structure\n",
        "- **Total observations**: ~12,100 rows (100 firms × 121 days)\n",
        "- **Raw features**: 7 columns (plus identifiers)\n",
        "- **Target variable**: `ret` (returns)\n",
        "- **Time period**: Covers 121 business days\n",
        "\n",
        "## Column Descriptions\n",
        "\n",
        "### Identifiers\n",
        "- **`date`**: Trading date (assuming all days are trading days -- in reality weekends and holidays should be excluded)\n",
        "- **`firm_id`**: Unique identifier for each firm (0-99)\n",
        "\n",
        "### Macroeconomic Variables\n",
        "- **`macro1`**: Categorical macroeconomic regime indicator with five possible states: 'Expansion', 'Contraction', 'Recovery', 'Peak', 'Trough'\n",
        "- **`macro2`**: Continuous macroeconomic factor, standardized values capturing quantitative aspects of economic environment\n",
        "\n",
        "### Firm-Specific Variables\n",
        "- **`price`**: Average daily price of the firm's stock (in currency units)\n",
        "- **`firm1`**: First firm-specific characteristic (standardized), could represent factors like size, liquidity, or operational metrics\n",
        "- **`firm2`**: Second firm-specific characteristic (standardized), capturing different firm attributes\n",
        "- **`firm3`**: Third firm-specific characteristic (standardized), representing additional firm-level factors\n",
        "\n",
        "### Target Variable\n",
        "- **`ret`**: Daily stock return for the firm, calculated as the percentage change in stock value\n",
        "\n",
        "## Data Characteristics\n",
        "\n",
        "### Time Series Properties\n",
        "- Data is ordered chronologically by date\n",
        "- Each firm appears once per trading day\n",
        "- Macroeconomic factors are the same across all firms on a given date\n",
        "- Macro1 shows regime persistence (economic states tend to persist over time)\n",
        "- Firm-specific characteristics and prices vary by both firm and date\n",
        "\n",
        "### Statistical Properties\n",
        "- Returns are approximately centered around zero with fat-tailed distribution\n",
        "- Macroeconomic regime (macro1) shows realistic persistence between states\n",
        "- Continuous macro factor (macro2) and firm characteristics are standardized\n",
        "- Prices are in absolute currency units (not standardized)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Train-Validation Split Strategy\n",
        "**Use Time-Based Splitting Only**\n",
        "\n",
        "Since this is time series data, you **MUST NOT** use random train-test splits. Instead, use a temporal cutoff to maintain the chronological order:\n",
        "\n",
        "```python\n",
        "# Example: Use first 80% of time period for training, last 20% for validation -- you can use different cuts\n",
        "cutoff_date = df['date'].quantile(0.8)\n",
        "train_data = df[df['date'] <= cutoff_date]\n",
        "val_data = df[df['date'] > cutoff_date]\n",
        "```\n",
        "\n",
        "Alternative approach using specific date:\n",
        "```python\n",
        "# Set a specific cutoff date -- you can use different cuts\n",
        "cutoff_date = pd.to_datetime('2020-03-01')  # Adjust based on your data range\n",
        "train_data = df[df['date'] <= cutoff_date]\n",
        "val_data = df[df['date'] > cutoff_date]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### Modeling Considerations\n",
        "\n",
        "**Feature Engineering Required**: This dataset contains only raw features. You will need to construct higher-level features such as:\n",
        "\n",
        "- **Lagged variables**: Create previous days' values for all features\n",
        "- **Interaction terms**: Construct products of features (e.g., macro1×macro2, firm1×firm2)\n",
        "- **Polynomial features**: Add squared, cubed terms for non-linear relationships\n",
        "- **Price transformations**: Log returns, price ratios, moving averages\n",
        "- **Rolling statistics**: Moving averages, volatility measures, momentum indicators\n",
        "- **Categorical encoding**: Proper encoding of macro1 (one-hot, label encoding, or embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLJEj1glXTtX"
      },
      "source": [
        "# Assignment 1: Financial Return Prediction using Elastic Net (20pts)\n",
        "\n",
        "## Overview\n",
        "You are provided with a financial time series dataset (`training_data.parquet`) containing daily observations for multiple firms. Your task is to build a predictive model for stock returns using Elastic Net regression and evaluate its performance.\n",
        "\n",
        "## Dataset Information\n",
        "- **File**: `training_data.parquet`\n",
        "- **Structure**: Panel data with firms observed over time\n",
        "- **Target Variable**: `ret` (daily stock returns)\n",
        "- **Features**: Raw financial and macroeconomic variables (see column names in the dataset)\n",
        "\n",
        "## Part 1: Elastic Net Model Implementation\n",
        "\n",
        "### Suggested steps:\n",
        "\n",
        "1. **Data Loading and Exploration**\n",
        "   - Load the training dataset from `training_data.parquet`\n",
        "   - Perform exploratory data analysis to understand the data structure\n",
        "   - Identify and handle any missing values or data quality issues\n",
        "\n",
        "2. **Feature Engineering**\n",
        "   - You may create additional features from the raw variables (interactions, lags, transformations)\n",
        "   - Ensure proper handling of categorical variables if present\n",
        "   - Consider financial domain knowledge in your feature construction\n",
        "\n",
        "3. **Train-Validation Split**\n",
        "   - Use time-based splitting (NOT random splitting)\n",
        "   - Split the data chronologically (e.g., use first 80% of time period for training, remaining 20% for validation)\n",
        "   - Justify your choice of split point\n",
        "\n",
        "4. **Model Development**\n",
        "   - Implement ElasticNet for return prediction\n",
        "   - Your final model must accept the same raw input feature names as in the training data\n",
        "   - The model should output a single prediction for the target variable `ret`\n",
        "\n",
        "5. **Hyperparameter Tuning**\n",
        "   - Tune the following hyperparameters:\n",
        "     - `alpha` (regularization strength)\n",
        "     - `l1_ratio` (balance between L1 and L2 regularization)\n",
        "   - Use appropriate validation strategy (time-series aware)\n",
        "   - Document your hyperparameter search process and final choices\n",
        "\n",
        "\n",
        "\n",
        "### Deliverables for Part 1:\n",
        "- Python code implementing the complete pipeline\n",
        "- A trained model object that can make predictions on new data\n",
        "- Documentation of your feature engineering choices\n",
        "- Justification for hyperparameter selection\n",
        "\n",
        "### Code Structure Requirements:\n",
        "```python\n",
        "# Your final model should work like this:\n",
        "import pandas as pd\n",
        "\n",
        "# Load new data with same column names as training data\n",
        "new_data = pd.read_parquet('new_data.parquet')\n",
        "\n",
        "# Make predictions\n",
        "predictions = your_model.predict(new_data)  # Should output 'ret' predictions\n",
        "```\n",
        "\n",
        "## Part 2: Feature Importance Analysis\n",
        "\n",
        "### Requirements:\n",
        "\n",
        "1. **Permutation-Based Feature Importance**\n",
        "   - Implement permutation feature importance on your best-performing model\n",
        "   - Calculate importance scores for each feature in your final model\n",
        "   - Use your validation set for this analysis\n",
        "\n",
        "2. **Statistical Significance Testing**\n",
        "   - For each feature, conduct statistical tests to determine if the importance score is significantly different from zero\n",
        "\n",
        "   - Report the p-value of each feature's importance\n",
        "\n",
        "3. **Results Interpretation**\n",
        "   - Rank features by their importance scores\n",
        "   - Identify which features are statistically significant predictors\n",
        "   - Provide economic/financial interpretation of your findings\n",
        "   - Discuss any surprising results\n",
        "\n",
        "### Deliverables for Part 2:\n",
        "- Implementation of permutation-based importance calculation\n",
        "- Statistical significance tests for each feature\n",
        "- Visualization of feature importance with statistical significance indicators\n",
        "- Written interpretation of results (2-3 paragraphs)\n",
        "\n",
        "## Evaluation Criteria\n",
        "\n",
        "Your model will be evaluated based on:\n",
        "\n",
        "1. **Predictive Performance (70%)**\n",
        "   - Performance on a hidden test set from the time period immediately following your training data\n",
        "   - Primary metric: Out-of-sample R² score\n",
        "   - Secondary metrics: RMSE and directional accuracy\n",
        "\n",
        "2. **Technical Implementation (15%)**\n",
        "   - Correct implementation of time-series validation\n",
        "   - Code quality and documentation\n",
        "\n",
        "3. **Feature Importance Analysis (15%)**\n",
        "   - Correct implementation of permutation importance\n",
        "   - Appropriate statistical testing\n",
        "   - Quality of interpretation and economic reasoning\n",
        "\n",
        "## Submission Requirements\n",
        "\n",
        "Submit the following files:\n",
        "\n",
        "1. **`main.py`** or **`main.ipynb`**: Complete implementation\n",
        "2. **`model.pkl`**: Your trained final model (saved using pickle or joblib)\n",
        "3. **`feature_importance_results.csv`**: Feature importance scores with significance tests\n",
        "4. **`report.pdf`**: a short report summarizing your approach, findings, and interpretations\n",
        "5. **`requirements.txt`**: a list of packages used and the corresponding versions. This is for replication purposely.\n",
        "\n",
        "**important note**: If your model cannot be run (be loaded using load pickle and do .predict with the test data), you would receive the lowest tier of rankings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObPsrOEvmZIc"
      },
      "source": [
        "# Assignment 2: Financial Return Prediction using XGBoost (20pts)\n",
        "\n",
        "Repeat the first exercise using XGBoost instead of ElasticNet. For feature importance use the number of times a feature is used to split across all trees. Use bootstrapping to estimate the p-values of feature importance scores.\n",
        "\n",
        "The deliverables are the same as the previous exercise with the same grading."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
